{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "available-california",
   "metadata": {},
   "source": [
    "## Language Identification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-generation",
   "metadata": {},
   "source": [
    "### 1. Estimate the prior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "english_files = [f\"./languageID/e{i}.txt\" for i in range(10)]\n",
    "japanese_files = [f\"./languageID/j{i}.txt\" for i in range(10)]\n",
    "spanish_files = [f\"./languageID/s{i}.txt\" for i in range(10)]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': english_files + japanese_files + spanish_files,\n",
    "    'label': ['e'] * len(english_files) + ['j'] * len(japanese_files) + ['s'] * len(spanish_files)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "total_docs = df.shape[0]\n",
    "\n",
    "# Compute prior probabilities\n",
    "prior_english = (df[df['label'] == 'e'].shape[0] + alpha) / (total_docs + 3 * alpha)\n",
    "prior_japanese = (df[df['label'] == 'j'].shape[0] + alpha) / (total_docs + 3 * alpha)\n",
    "prior_spanish = (df[df['label'] == 's'].shape[0] + alpha) / (total_docs + 3 * alpha)\n",
    "\n",
    "print(f\"Prior probability for English: {prior_english}\")\n",
    "print(f\"Prior probability for Japanese: {prior_japanese}\")\n",
    "print(f\"Prior probability for Spanish: {prior_spanish}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-disco",
   "metadata": {},
   "source": [
    "### 2. Estimate the class conditional probability for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(97, 123)] + [' ']\n",
    "\n",
    "char_counts = {char: 0 for char in chars}\n",
    "\n",
    "# Counts of each character in English documents\n",
    "for file in english_files:\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read().lower()\n",
    "        for char in chars:\n",
    "            char_counts[char] += text.count(char)\n",
    "\n",
    "total_chars = sum(char_counts.values())\n",
    "\n",
    "theta_e = {char: round((count + alpha) / (total_chars + 27 * alpha),3) for char, count in char_counts.items()}\n",
    "theta_e.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-playing",
   "metadata": {},
   "source": [
    "### 3. Estimate the conditional probabilities for spanish and janpanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_theta(files, chars, alpha):\n",
    "    char_counts = {char: 0 for char in chars}\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read().lower()\n",
    "            for char in chars:\n",
    "                char_counts[char] += text.count(char)\n",
    "                \n",
    "    total_chars = sum(char_counts.values())\n",
    "    \n",
    "    return {char: round((count + alpha) / (total_chars + 27 * alpha),3) for char, count in char_counts.items()}\n",
    "\n",
    "theta_j = calculate_theta(japanese_files, chars, alpha)\n",
    "theta_s = calculate_theta(spanish_files, chars, alpha)\n",
    "\n",
    "print(theta_j.values())\n",
    "print(theta_s.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-aside",
   "metadata": {},
   "source": [
    "### 4. Bag-of-words vector x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = {char: 0 for char in chars}\n",
    "\n",
    "test_file_path = f\"./languageID/e10.txt\"\n",
    "with open(test_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "for char in chars:\n",
    "    bow_vector[char] = text.count(char)\n",
    "\n",
    "bow_series = pd.Series(bow_vector)\n",
    "print(bow_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-dinner",
   "metadata": {},
   "source": [
    "### 5. Compute px given y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_px_given_y(bow_vector, theta):\n",
    "    probabilities = [theta[char]**count for char, count in bow_vector.items()]\n",
    "    return np.prod(probabilities)\n",
    "\n",
    "p_x_given_e = compute_px_given_y(bow_vector, theta_e)\n",
    "p_x_given_j = compute_px_given_y(bow_vector, theta_j)\n",
    "p_x_given_s = compute_px_given_y(bow_vector, theta_s)\n",
    "\n",
    "print(f\"p(x|y=e): {p_x_given_e}\")\n",
    "print(f\"p(x|y=j): {p_x_given_j}\")\n",
    "print(f\"p(x|y=s): {p_x_given_s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-economy",
   "metadata": {},
   "source": [
    "### 6. Use Bayes rule and your estimated prior and likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportional posterior p(y|x) for each class\n",
    "prior_e = 1/3\n",
    "prior_j = 1/3\n",
    "prior_s = 1/3\n",
    "\n",
    "p_x = proportional_posterior_e + proportional_posterior_j + proportional_posterior_s\n",
    "\n",
    "proportional_posterior_e = p_x_given_e * prior_e / p_x\n",
    "proportional_posterior_j = p_x_given_j * prior_j / p_x\n",
    "proportional_posterior_s = p_x_given_s * prior_s / p_x\n",
    "\n",
    "p_y_given_x_e = proportional_posterior_e\n",
    "p_y_given_x_j = proportional_posterior_j\n",
    "p_y_given_x_s = proportional_posterior_s\n",
    "predicted_class = max([(p_y_given_x_e, \"e\"), (p_y_given_x_j, \"j\"), (p_y_given_x_s, \"s\")], key=lambda x: x[0])[1]\n",
    "\n",
    "print(f\"p(y=e|x): {p_y_given_x_e}\")\n",
    "print(f\"p(y=j|x): {p_y_given_x_j}\")\n",
    "print(f\"p(y=s|x): {p_y_given_x_s}\")\n",
    "print(f\"Predicted class label of x: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-lambda",
   "metadata": {},
   "source": [
    "### 7. The performance of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"e\", \"j\", \"s\"]\n",
    "confusion_matrix = pd.DataFrame(0, index=languages, columns=languages)\n",
    "\n",
    "def classify_document(file_path):\n",
    "    bow_vector = {char: 0 for char in chars}\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read().lower()\n",
    "    for char in chars:\n",
    "        bow_vector[char] = text.count(char)\n",
    "\n",
    "    p_e = compute_px_given_y(bow_vector, theta_e) * prior_e\n",
    "    p_j = compute_px_given_y(bow_vector, theta_j) * prior_j\n",
    "    p_s = compute_px_given_y(bow_vector, theta_s) * prior_s\n",
    "\n",
    "    return max([(p_e, \"e\"), (p_j, \"j\"), (p_s, \"s\")], key=lambda x: x[0])[1]\n",
    "\n",
    "for true_language in languages:\n",
    "    for i in range(10, 20):\n",
    "        file_path = f\"./languageID/{true_language}{i}.txt\"\n",
    "        predicted_language = classify_document(file_path)\n",
    "        confusion_matrix.at[predicted_language, true_language] += 1\n",
    "\n",
    "# Display confusion matrix\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-fortune",
   "metadata": {},
   "source": [
    "## Simple Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(x, W1, W2, W3):\n",
    "    z1 = np.dot(x, W1.T)\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2, W3.T)\n",
    "    y_hat = softmax(z3)\n",
    "    return z1, a1, z2, a2, y_hat\n",
    "\n",
    "def backward_propagation(x, y, W1, W2, W3, z1, a1, z2, a2, y_hat, alpha):\n",
    "    m = x.shape[0]\n",
    "    dz3 = y_hat - y\n",
    "    dW3 = 1/m * np.dot(dz3.T, a2)\n",
    "    \n",
    "    da2 = np.dot(dz3, W3)\n",
    "    dz2 = da2 * sigmoid_derivative(a2)\n",
    "    dW2 = 1/m * np.dot(dz2.T, a1)\n",
    "    \n",
    "    da1 = np.dot(dz2, W2)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = 1/m * np.dot(dz1.T, x)\n",
    "    \n",
    "    W1 -= alpha * dW1\n",
    "    W2 -= alpha * dW2\n",
    "    W3 -= alpha * dW3\n",
    "    return W1, W2, W3\n",
    "\n",
    "d = 28 * 28\n",
    "d1 = 300\n",
    "d2 = 200\n",
    "k = 10\n",
    "alpha = 0.1\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "W1 = np.random.randn(d1, d) * 0.01\n",
    "W2 = np.random.randn(d2, d1) * 0.01\n",
    "W3 = np.random.randn(k, d2) * 0.01\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        x = images.view(-1, 28*28).numpy()\n",
    "        y = np.eye(10)[labels.numpy()]\n",
    "        \n",
    "        z1, a1, z2, a2, y_hat = forward_propagation(x, W1, W2, W3)\n",
    "        W1, W2, W3 = backward_propagation(x, y, W1, W2, W3, z1, a1, z2, a2, y_hat, alpha)\n",
    "        \n",
    "        total_loss += -np.mean(y * np.log(y_hat + 1e-8))\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
    "\n",
    "# the learning curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()\n",
    "\n",
    "corrects = 0\n",
    "for images, labels in test_loader:\n",
    "    x = images.view(-1, 28*28).numpy()\n",
    "    y = np.eye(10)[labels.numpy()]\n",
    "    _, _, _, _, y_hat = forward_propagation(x, W1, W2, W3)\n",
    "    corrects += np.sum(np.argmax(y_hat, axis=1) == labels.numpy())\n",
    "\n",
    "accuracy = corrects / len(test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = NeuralNetwork(28*28, 300, 200, 10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_zero(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.constant_(m.weight, 0)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model_zero = NeuralNetwork(28*28, 300, 200, 10)\n",
    "model_zero.apply(weight_init_zero)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_zero.parameters(), lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_zero(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()\n",
    "\n",
    "model_zero.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = model_zero(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_random(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.uniform_(m.weight, -1, 1)\n",
    "        nn.init.uniform_(m.bias, -1, 1)\n",
    "\n",
    "model_random = NeuralNetwork(28*28, 300, 200, 10)\n",
    "model_random.apply(weight_init_random)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_random.parameters(), lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_random(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()\n",
    "\n",
    "model_random.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = model_random(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-document",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
